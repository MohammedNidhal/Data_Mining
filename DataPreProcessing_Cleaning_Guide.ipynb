{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hu2rTPqeQ6hA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd    # to handle the dataframe\n",
        "pd.set_option('expand_frame_repr', False) #to avoid the multi-lines formatting of the dataframe\n",
        "import numpy as np     # to handle numbers and ndarray\n",
        "import missingno as msno #to visualize missing data and get a quick visual summary of the completeness of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('CSVFileName.csv')\n"
      ],
      "metadata": {
        "id": "xJgjrS4bTwKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Values"
      ],
      "metadata": {
        "id": "9ek757ydT6aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of missing values per column')\n",
        "for colm in df.columns:\n",
        "  n_mv=df[colm].isnull().sum()\n",
        "  print(f'{colm}: {n_mv}')"
      ],
      "metadata": {
        "id": "j165xGB7T3XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df[colm].isnull() returns how many instances are null in the given column(colm)"
      ],
      "metadata": {
        "id": "b-kWPwtYUAM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visulasation of Missing values"
      ],
      "metadata": {
        "id": "r9ppL_QUUKae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msno.bar(df)"
      ],
      "metadata": {
        "id": "74vflag8T3U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Barplots"
      ],
      "metadata": {
        "id": "mVPa4PIyUQhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "yzM1vIVGT3S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allows to visualize the absence and missing values using a matrix"
      ],
      "metadata": {
        "id": "1_qaxqaUUSvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Missing values** : These Gaphs may help you in building a feedback on why data are mising"
      ],
      "metadata": {
        "id": "dCIK0XkqUhHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msno.heatmap(df)"
      ],
      "metadata": {
        "id": "bAc86dKaT3QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msno.dendrogram(df)"
      ],
      "metadata": {
        "id": "RcUUEfEaT3OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting missing values"
      ],
      "metadata": {
        "id": "We0-LBTZU6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deletions\n",
        "#Listwise deleltions\n",
        "df_wmv\n",
        "df_wmv.dropna(subset=['Column'],how='any',inplace=True)\n",
        "f_wmv['Column'].isnull().sum() # A call to verify if there is still null values or not"
      ],
      "metadata": {
        "id": "0fty9CJVVQ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deletes all the null instances with null value in the given **Column**"
      ],
      "metadata": {
        "id": "Vi3-ofQsVRmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deletions\n",
        "#Dropping the entire columns\n",
        "df_wmv.drop('Column',axis=1,inplace=True)\n",
        "df_wmv"
      ],
      "metadata": {
        "id": "TONZmIUcVlv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This action deletes the whole **Column** from the dataset that's why it is not recommanded to use it unless the number of missing values in that data set is **>80%**"
      ],
      "metadata": {
        "id": "m-CbdvzcVqIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputations for non time series data\n",
        "# imputing with a constant\n",
        "from sklearn.impute import SimpleImputer\n",
        "df_const = df.copy()\n",
        "#setting strategy to 'constant'\n",
        "const_imputer = SimpleImputer(strategy='constant') # imputing using constant value\n",
        "df_const.iloc[:,:] = const_imputer.fit_transform(df_const)\n",
        "df_const.isnull().sum()"
      ],
      "metadata": {
        "id": "xxhUa2QvVlte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This methode replaces all null values in the whole dataset with a constant:\n",
        "Usually we don't use this method as it doesn't give any advantage when dealinmg with missing data it only says the missing data are here"
      ],
      "metadata": {
        "id": "nM-kkG0RWKo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputations for non time series data\n",
        "# imputing with general central tendency\n",
        "df_centTend = df.copy()\n",
        "#setting strategy to 'most_frequent' to impute by the mode\n",
        "md_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median, by default mean\n",
        "df_centTend.iloc[:,:] = md_imputer.fit_transform(df_centTend)\n",
        "df_centTend.isnull().sum() # TO CHECK IF WE MADE THE RIGGHT WORK OR NOT"
      ],
      "metadata": {
        "id": "63E7LnPlVlrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the most basic sloution as it allows to handel data and give them real results that can be used later inm interpretations ***(Mean dOESN'T WORK ON CATEGORICAL DATA)***"
      ],
      "metadata": {
        "id": "h5YZgKjBWkMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****Using KNN technique (Advanced Technique)*****"
      ],
      "metadata": {
        "id": "pXOalBGlXCpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Advanced imputation techniques\n",
        "#Using KNN\n",
        "num_split=df.loc[:,df.dtypes!=object] # This allows us to extract onlt the numerical columns (object stands for string which we will be ignoring in order to complete KNN data cleaning)\n",
        "num_split_knn= num_split.copy()\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=3, metric='nan_euclidean', weights='uniform') #By default parameters except the n_neighbors changed to 3 as it was 5\n",
        "num_split_knn.iloc[:,:] = imputer.fit_transform(num_split_knn)\n",
        "num_split_knn.isnull().sum()"
      ],
      "metadata": {
        "id": "l0sB93hnVlpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to fill out the missing values we use clustering in order to minimaize the frequencey of using mean or median. it allows for more data diversity and helps in having better data quilty"
      ],
      "metadata": {
        "id": "ohMg3KOzXYpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****Using MICE (Advanced Technique)*****"
      ],
      "metadata": {
        "id": "ihlEJvm4XsQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Advanced imputation techniques\n",
        "#Using MICE\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer #Multivariate imputer that estimates each feature from all the others, as a function of other features in a round-robin fashion\n",
        "df_mice = df.copy()\n",
        "\n",
        "mice_imputer = IterativeImputer()\n",
        "df_mice['Column'] = mice_imputer.fit_transform(df_mice[['Column']])\n",
        "df_mice.isnull().sum()"
      ],
      "metadata": {
        "id": "QwLSR0qKVlm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Column must be numerical and you can do this technique for how many number of numerical columns you want just replace **Column** with your wanted **Column from the dataframe**"
      ],
      "metadata": {
        "id": "DBh-XHKFX67P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dealing With Noise and Outliers**"
      ],
      "metadata": {
        "id": "08NfSwVhYTl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization of data curves of the numerical attributes\n",
        "for column in df_centTend.columns:\n",
        "  df_centTend[column].plot()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ju6dcrNHYKml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization with scatter plots\n",
        "df_centTend.plot.scatter(x='Column1', y='Column2')"
      ],
      "metadata": {
        "id": "JLmS2YksayJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization of scatter plot for all the columns\n",
        "pd.plotting.scatter_matrix(df_centTend, alpha=0.2)"
      ],
      "metadata": {
        "id": "e2n57vCUayG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Visualization with the best fit (\"trend\" line) for a guided understanding and explanation\n",
        "#plot the data points\n",
        "df_centTend.plot.scatter(x='Column1', y='Column2', color='DarkBlue')\n",
        "#Fit a line to the data\n",
        "x=np.array(df_centTend['Column1'])\n",
        "y=np.array(df_centTend['Column2'])\n",
        "a, b = np.polyfit(x, y, 1) #calculating the slope a and the y_intercept b of the line\n",
        "plt.plot(x, a*x + b, color='red', label='Line of Best Fit') #a linear regression line"
      ],
      "metadata": {
        "id": "KQCw90P_ayEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmaps, correlation study and noise\n",
        "sns.heatmap(df_centTend)"
      ],
      "metadata": {
        "id": "MJK6HbI7ayB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmaps, correlation study and noise\n",
        "sns.heatmap(df_centTend.corr(), annot=True)"
      ],
      "metadata": {
        "id": "phzAoZm3ax_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Smoothing noisy data with bining\n",
        "#Smoothing by bin mean\n",
        "import random\n",
        "import statistics\n",
        "from scipy.stats import binned_statistic\n",
        "\n",
        "num_bins = n\n",
        "# Use binned_statistic to calculate mean within each bin\n",
        "result = binned_statistic(df_centTend['Column'], df_centTend['Column'], bins=num_bins, statistic='mean') #statistic is by default mean, you can choose median to perform smoothing by bin median, or min, max to smooth bin boundaries\n",
        "\n",
        "# Extract bin edges and binned mean from the result\n",
        "bin_edges = result.bin_edges\n",
        "bin_means = result.statistic\n",
        "bin_indices= result.binnumber\n",
        "hist=np.bincount(bin_indices)\n",
        "\n",
        "print('Bin Edges: ', bin_edges)\n",
        "print('Binned Mean: ', bin_means)\n",
        "print('Bin Indices: ', hist)\n",
        "\n",
        "plt.hist(bin_edges, weights=hist)"
      ],
      "metadata": {
        "id": "_ltw21aibbuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n : You need  to specify the number of bins (recomandeeed between 5 and 20)\n",
        "\n",
        "Column : a numerical column that we want to check the noise and outliers in"
      ],
      "metadata": {
        "id": "wfSWP56Hbs-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Smoothing noisy data with bining\n",
        "#Equal-width Binning\n",
        "# Define the number of bins\n",
        "num_bins = 3\n",
        "# Use numpy's histogram function for equal width bins\n",
        "hist, bins = np.histogram(df_centTend['GStat'], bins=num_bins)\n",
        "print('Bin Edges: ', bins)\n",
        "print('Histogram Counts: ', hist)\n",
        "plt.stairs(hist, bins, fill=True)"
      ],
      "metadata": {
        "id": "DyiYKiGMbbsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is another type of binning called **Equal-width Binning**"
      ],
      "metadata": {
        "id": "ti4ode_qcCBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Smoothing the data with lowess smoothing\n",
        "import statsmodels.api as sm\n",
        "x=np.array(df_centTend['Column1'])\n",
        "y=np.array(df_centTend['Column2'])\n",
        "lowess = sm.nonparametric.lowess(y, x, frac=0.3)\n",
        "plt.scatter(x, y)\n",
        "plt.plot(lowess[:, 0], lowess[:, 1], c='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OZHFAYTIbbp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Smoothing data with clustering**"
      ],
      "metadata": {
        "id": "U0GZQtFMcgRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Smoothing noisy data with clustering\n",
        "#Importing additional libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Transform the data with Principal components analysis (dimensionality reduction technique) before applying Kmeans\n",
        "pca = PCA(2) #Only keep two components\n",
        "df = pca.fit_transform(df_centTend.loc[:, ['COLUMN1','...COLUMN_N']])\n",
        "df.shape"
      ],
      "metadata": {
        "id": "R2i7E1uLca98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**['COLUMN1','...COLUMN_N']** : ALL of them must be numerical columns"
      ],
      "metadata": {
        "id": "K_pZS3_wcvGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instanciating kmeans with a given number of clusters\n",
        "kmeans = KMeans(n_clusters= 3)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df)\n",
        "\n",
        "print(label)"
      ],
      "metadata": {
        "id": "GT6HIxyYbbnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the clusters predection"
      ],
      "metadata": {
        "id": "LxNulRFYdFbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the clusters\n",
        "#Getting unique labels\n",
        "u_labels = np.unique(label)\n",
        "#plotting the results:\n",
        "for i in u_labels:\n",
        "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OAC6K9rObblE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3q3pnjuodN45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the Centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "u_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "\n",
        "for i in u_labels:\n",
        "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
        "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "67pJdCekdNP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the centroids and the whole data in a scatterplot to better help in understanding where the mean values for each class are located"
      ],
      "metadata": {
        "id": "lOb2gbIddPKO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3p1ElKBedWen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}