{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr1GGgkUAhW3"
      },
      "outputs": [],
      "source": [
        "#Getting set up\n",
        "import pandas as pd #to handle the data frame\n",
        "from sklearn.impute import SimpleImputer #to impute the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder # binarisation of the categorical attributes\n",
        "from sklearn.preprocessing import StandardScaler #implements the z-score\n",
        "from sklearn.decomposition import PCA #for dimensionality reduction after the transformations made\n",
        "from sklearn import metrics #to evaluate the quality of clustering\n",
        "import numpy as np #to convert the dataframe into muldimensional arrays and work with\n",
        "import matplotlib.pyplot as plt #for plotting and visualization\n",
        "import sklearn.cluster as cluster #Sklearn has 13 clustering algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most used and frequent libraries in clustering tasks"
      ],
      "metadata": {
        "id": "n1YyOuG-ArVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('CSVFile.csv')"
      ],
      "metadata": {
        "id": "uhy4l0XjAuwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_num=df.loc[:,tayou_df.dtypes!=object]\n",
        "preprocess_cat=df.loc[:,tayou_df.dtypes==object]"
      ],
      "metadata": {
        "id": "VFOtwvWkNpSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing the data\n",
        "#filling in the missing values\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "preprocess_num = df[num_vars].to_numpy()\n",
        "preprocess_num=num_imputer.fit_transform(preprocess_num)\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "preprocess_cat = df[cat_vars].to_numpy()\n",
        "preprocess_cat=cat_imputer.fit_transform(preprocess_cat)\n",
        "#Handling categorical variables\n",
        "cat_encoder = OneHotEncoder()\n",
        "preprocess_cat = cat_encoder.fit_transform(preprocess_cat)\n",
        "preprocess_cat = preprocess_cat.toarray()\n",
        "print(preprocess_cat)"
      ],
      "metadata": {
        "id": "FgO2AAdWA1hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deviding data into categorical and Numerical as they have different tratment"
      ],
      "metadata": {
        "id": "E9WNydVIBDse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_num=StandardScaler().fit_transform(preprocess_num)"
      ],
      "metadata": {
        "id": "f6PEI0mQBIBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical Data normalization"
      ],
      "metadata": {
        "id": "Zc3MlPUTBLEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessed=np.column_stack((preprocess_cat, preprocess_num))"
      ],
      "metadata": {
        "id": "-5tdWiWbBM7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conncatination of Data after pre-process"
      ],
      "metadata": {
        "id": "PIsl-Q_NBNOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensionlaity reduction with PCA\n",
        "pca = PCA(36, svd_solver=\"covariance_eigh\")\n",
        "df_reduced = pca.fit_transform(df_preprocessed)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ],
      "metadata": {
        "id": "w5Wsj8BfBY_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce dimensinality to ensure that data is reduced so we can have better calculation time"
      ],
      "metadata": {
        "id": "Ri3keKWEGzbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmeans\n",
        "#finidng the best k, number of clusters\n",
        "#first method, elbow method\n",
        "wcss = [] #initializing the variable that will contain the Within Cluster Sum of Squares computed at each iteration (The Y-axis of the curve)\n",
        "max_clusters = min(10, len(df_reduced))\n",
        "for i in range(1, max_clusters + 1): #to run the kmeans for different cluster numbers ranging from 1 to 10 (The X-axis of the curve)\n",
        "    kmeans = cluster.KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(df_reduced)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plotting the Elbow Method graph\n",
        "plt.plot(range(1, max_clusters + 1), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBmpJBXtG62h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the elbow methode to know what is the best number of clusters\n",
        "\n",
        "The best number of clusters is simply the inflection point: https://www.google.com/imgres?q=elbow%20method%20for%20clustering&imgurl=https%3A%2F%2Fbuiltin.com%2Fsites%2Fwww.builtin.com%2Ffiles%2Fstyles%2Fckeditor_optimize%2Fpublic%2Finline-images%2F1_elbow-method.jpeg&imgrefurl=https%3A%2F%2Fbuiltin.com%2Fdata-science%2Felbow-method&docid=ddvzSng-N0TI6M&tbnid=-Lpc8W4OBEjPDM&vet=12ahUKEwiewa77m9WJAxVwTqQEHTneDzQQM3oECBsQAA..i&w=512&h=275&hcb=2&ved=2ahUKEwiewa77m9WJAxVwTqQEHTneDzQQM3oECBsQAA\n",
        "\n",
        "POINT WHERE THE wcss gets stable"
      ],
      "metadata": {
        "id": "CKi67g2oHDB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmeans\n",
        "#finidng the best k, number of clusters\n",
        "#first method, elbow method, second version with an indication\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "model = cluster.KMeans(random_state=42)\n",
        "visualizer = KElbowVisualizer(model, k=(2,10))\n",
        "visualizer.fit(df_reduced)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "QyadaWhmH1Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using both WCSS and we can know the best score"
      ],
      "metadata": {
        "id": "kRAFXxYbIeOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmeans\n",
        "#finidng the best k, number of clusters\n",
        "#second method, silhouette coefficient\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "fig, ax = plt.subplots(3, 2, figsize=(15,8))\n",
        "for i in [2, 3, 4, 5]:\n",
        "#Create KMeans instances for different number of clusters\n",
        "  km = cluster.KMeans(n_clusters=i, random_state=42)\n",
        "  q, mod = divmod(i, 2)\n",
        "#Create SilhouetteVisualizer instance with KMeans instance\n",
        "#Fit the visualizer\n",
        "  visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
        "  visualizer.fit(df_reduced)"
      ],
      "metadata": {
        "id": "_L58yTjUIq6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us the best idea on how the data must be distrubuted (the best K ) as we calculate the Silhouette score. The best number of clusters is the one with the biggest Silhouette score"
      ],
      "metadata": {
        "id": "B2AvXwodI5lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the class object\n",
        "kmeans = cluster.KMeans(n_clusters= 3, random_state=42)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_reduced)\n",
        "\n",
        "print(label)"
      ],
      "metadata": {
        "id": "6xdvFjSQJHMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you you will ned 3 into the conviniet number of clusters which we found after doing the previous operations"
      ],
      "metadata": {
        "id": "x7L3s8IpJQmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the clusters\n",
        "#Getting unique labels\n",
        "u_labels = np.unique(label)\n",
        "#plotting the results:\n",
        "for i in u_labels:\n",
        "    plt.scatter(df_reduced[label == i , 0] , df_reduced[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MXOSvABoJwz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disperssion of Data using visuail approch to know how our clusters have been created"
      ],
      "metadata": {
        "id": "6qMa_CYPJy4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the Centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "u_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "\n",
        "for i in u_labels:\n",
        "    plt.scatter(df_reduced[label == i , 0] , df_reduced[label == i , 1] , label = i)\n",
        "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SMQ4NtsVJ4T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as previous cell but it includes centroids to know the mean tendancy"
      ],
      "metadata": {
        "id": "PQrbHJwNKA7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation of the clustering\n",
        "print(f\"Silhouette Coefficient for kmeans: {metrics.silhouette_score(df_reduced, label):.3f}\")"
      ],
      "metadata": {
        "id": "vAIxJrHsKGPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric to be used Silhouette score"
      ],
      "metadata": {
        "id": "Vhxb63H1LazF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DBSCAN\n",
        "#Finding the optimal parameters\n",
        "# initialize the value of k for kNN which can be same as MinPts (rule of thumb: 2 * dimensionality of the data)\n",
        "k = 4\n",
        "# Compute k-nearest neighbors\n",
        "# you need to add 1 to k as this function also return distance to itself\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "nbrs = NearestNeighbors(n_neighbors=k+1).fit(df_reduced)\n",
        "\n",
        "# get distances\n",
        "dist, ind = nbrs.kneighbors(df_reduced)\n",
        "#drop the first column, and sort distances in an ascending order\n",
        "k_dist = np.sort(dist[:, -1])\n",
        "#Plot the kNN graph with sorted distance,\n",
        "plt.plot(k_dist)\n",
        "plt.xlabel('Distance sorted points')\n",
        "plt.ylabel(f'{k}-Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WA8IXWMVLgf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kneed\n",
        "from kneed import KneeLocator\n",
        "kneedle = KneeLocator(x = range(1, len(dist)+1), y = k_dist, S = 1.0,\n",
        "                      curve = \"concave\", direction = \"increasing\", online=True)\n",
        "\n",
        "# get the estimate of knee point\n",
        "print(kneedle.knee_y)"
      ],
      "metadata": {
        "id": "vxgT8MFFLn_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kneedle.plot_knee()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uJr2BTk3LsVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to chose the parametersof The DSBCAN : a density based clustering algorithm that takes disperssion in consedration"
      ],
      "metadata": {
        "id": "8GkMFg2ILp4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan=cluster.DBSCAN(eps=1.25, min_samples=4, metric='euclidean')\n",
        "label = dbscan.fit_predict(df_reduced)"
      ],
      "metadata": {
        "id": "qBspwCq2L-fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making DSBCAN predection , the hyper parameters will be detrmined using the 3 cells that preceed this one"
      ],
      "metadata": {
        "id": "wa8ZfQ9yL_OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the clusters\n",
        "#Getting unique labels\n",
        "u_labels = np.unique(label)\n",
        "#plotting the results:\n",
        "for i in u_labels:\n",
        "    plt.scatter(df_reduced[label == i , 0] , df_reduced[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cyoJDWP7LvV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot to see the differences\n",
        "\n",
        "if there is a negative value for a cluster than that cluster is highly an outliers cluster ... meaning that it has elements that are far away from the general characterstic from our data"
      ],
      "metadata": {
        "id": "tWmow4mtMIvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster evaluation\n",
        "print(f\"Silhouette Coefficient for DBSCAN: {metrics.silhouette_score(df_reduced, label):.3f}\")"
      ],
      "metadata": {
        "id": "vTAEd6aFMIj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score of this methode"
      ],
      "metadata": {
        "id": "8C0Hd4dIMm4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hierarchical clustering\n",
        "#Agglomerative clustering\n",
        "agglomerative = cluster.AgglomerativeClustering(n_clusters=3)\n",
        "labels = agglomerative.fit_predict(df_reduced)\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "KWpOP03dMoXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8xlbO8N0Murt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "# setting distance_threshold=0 ensures we compute the full tree.\n",
        "model = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None) #Set the distance threshold or n_clusters\n",
        "\n",
        "model = model.fit(df_reduced)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fSKiySa5MvRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the dendrogram it considers all elements as seprate classes and than staart mixing themm until we have one class\n",
        "\n",
        "There is a big misconception that we use it to deteect the right number of classes to use in KMEANS well it's a wrong approch so be careful"
      ],
      "metadata": {
        "id": "OmASIKT6My2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster evaluation\n",
        "print(f\"Silhouette Coefficient for agglomerative clustering: {metrics.silhouette_score(df_reduced, labels):.3f}\")"
      ],
      "metadata": {
        "id": "Rr4Mo2c3MzXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}